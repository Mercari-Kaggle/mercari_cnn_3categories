{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "215d83ee-fdd1-4940-b04b-0d30ed50048f",
    "_uuid": "3230615bf5d10ae9121d4ff2a4f305fe7950bbe7",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.layers import MaxPooling1D, Conv1D\n",
    "from subprocess import check_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ea119c35-0f26-4338-804e-96a4564a7e5a",
    "_uuid": "0c1b213c7799f675682ef703d19278305d27b88e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData = pd.read_csv('../input/train.tsv', sep='\\t')\n",
    "testData = pd.read_csv('../input/test.tsv', sep='\\t')\n",
    "print(trainData.shape)\n",
    "print(testData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d1a18d6c-922c-4a6d-bdaf-ce61c76b5b5c",
    "_uuid": "4b07893886c4b77a3d3105811c0393ea718b3bd9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#HANDLE MISSING VALUES\n",
    "print(\"Handling missing values...\")\n",
    "def handle_missing(dataset):\n",
    "    dataset.category_name.fillna(value=\"missing\", inplace=True)\n",
    "    dataset.brand_name.fillna(value=\"missing\", inplace=True)\n",
    "    dataset.item_description.fillna(value=\"missing\", inplace=True)\n",
    "    return (dataset)\n",
    "\n",
    "trainData = handle_missing(trainData)\n",
    "testData = handle_missing(testData)\n",
    "print(trainData.shape)\n",
    "print(testData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f9d5ea4d-9815-41f6-8d6d-13c0cf6ba558",
    "_uuid": "340a74e1470348199fb39e0a2517fbe876f2a705",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categoryAll = pd.DataFrame(trainData.category_name.str.split('/').tolist(),columns = ['category1','category2','category3','category4','category5'])\n",
    "trainData = pd.concat([trainData, categoryAll], axis=1)\n",
    "idx = []\n",
    "for i in range(len(trainData)):\n",
    "    if type(trainData['category4'][i]) == str:\n",
    "        idx.append(i)\n",
    "sep = \"&\"\n",
    "for i in idx:\n",
    "    if type(trainData['category5'][i]) == str:\n",
    "        combining = (trainData['category3'][i],trainData['category4'][i],trainData['category5'][i])\n",
    "        trainData['category3'][i] = sep.join(combining)\n",
    "    else:\n",
    "        combining = (trainData['category3'][i],trainData['category4'][i])\n",
    "        trainData['category3'][i] = sep.join(combining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0f032e15-47cd-403c-843a-815a9256a7bc",
    "_uuid": "2c0f0c59bf77f4ba8ff99074a6a6c33c9a8498dd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categoryAll = pd.DataFrame(testData.category_name.str.split('/').tolist(),columns = ['category1','category2','category3','category4','category5'])\n",
    "testData = pd.concat([testData, categoryAll], axis=1)\n",
    "idx = []\n",
    "for i in range(len(testData)):\n",
    "    if type(testData['category4'][i]) == str:\n",
    "        idx.append(i)\n",
    "sep = \"&\"\n",
    "for i in idx:\n",
    "    if type(testData['category5'][i]) == str:\n",
    "        combining = (testData['category3'][i],testData['category4'][i],testData['category5'][i])\n",
    "        testData['category3'][i] = sep.join(combining)\n",
    "    else:\n",
    "        combining = (testData['category3'][i],testData['category4'][i])\n",
    "        testData['category3'][i] = sep.join(combining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4a430929-e7c1-4509-9118-794ef4ac7b48",
    "_uuid": "cb075d4f2309c868fdf017dd00b0f417e1084d50",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData.category1.fillna(value=\"missing\", inplace=True)\n",
    "trainData.category2.fillna(value=\"missing\", inplace=True)\n",
    "trainData.category3.fillna(value=\"missing\", inplace=True)\n",
    "testData.category1.fillna(value=\"missing\", inplace=True)\n",
    "testData.category2.fillna(value=\"missing\", inplace=True)\n",
    "testData.category3.fillna(value=\"missing\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "49d911ba-5476-4578-b677-558ac8421339",
    "_uuid": "e36d9be8cf3469f47d3ee8616b41cea16b0318fb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(np.hstack([trainData.category1, testData.category1]))\n",
    "trainData.category1 = le.transform(trainData.category1)\n",
    "testData.category1 = le.transform(testData.category1)\n",
    "\n",
    "le.fit(np.hstack([trainData.category2, testData.category2]))\n",
    "trainData.category2 = le.transform(trainData.category2)\n",
    "testData.category2 = le.transform(testData.category2)\n",
    "\n",
    "le.fit(np.hstack([trainData.category3, testData.category3]))\n",
    "trainData.category3 = le.transform(trainData.category3)\n",
    "testData.category3 = le.transform(testData.category3)\n",
    "\n",
    "le.fit(np.hstack([trainData.brand_name, testData.brand_name]))\n",
    "trainData.brand_name = le.transform(trainData.brand_name)\n",
    "testData.brand_name = le.transform(testData.brand_name)\n",
    "del le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e686d93e-490e-45eb-aec8-103ec04bb56b",
    "_uuid": "4bab445c8b1a59a585c0236c14f4d59f9127c68a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "raw_text = np.hstack([trainData.item_description.str.lower(), trainData.name.str.lower()])\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "tok_raw = Tokenizer()\n",
    "tok_raw.fit_on_texts(raw_text)\n",
    "print(\"Transforming...\")\n",
    "\n",
    "trainData[\"seq_item_description\"] = tok_raw.texts_to_sequences(trainData.item_description.str.lower())\n",
    "testData[\"seq_item_description\"] = tok_raw.texts_to_sequences(testData.item_description.str.lower())\n",
    "trainData[\"seq_name\"] = tok_raw.texts_to_sequences(trainData.name.str.lower())\n",
    "testData[\"seq_name\"] = tok_raw.texts_to_sequences(testData.name.str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c6d05021-0f59-48ac-81de-5fb24b0a9152",
    "_uuid": "6e8ea5deafb046c95ac14d0c5e48db4c5262ba5c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_name_seq = np.max([np.max(trainData.seq_name.apply(lambda x: len(x))), np.max(testData.seq_name.apply(lambda x: len(x)))])\n",
    "max_seq_item_description = np.max([np.max(trainData.seq_item_description.apply(lambda x: len(x)))\n",
    "                                   , np.max(testData.seq_item_description.apply(lambda x: len(x)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "da162438-64a3-4af1-8740-96656301de5b",
    "_uuid": "d168402ad1299dc969fa3d979aa85d11044c703e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#EMBEDDINGS MAX VALUE\n",
    "#Base on the histograms, we select the next lengths\n",
    "MAX_NAME_SEQ = 10\n",
    "MAX_ITEM_DESC_SEQ = 50\n",
    "MAX_TEXT = np.max([np.max(trainData.seq_name.max())\n",
    "                   , np.max(testData.seq_name.max())\n",
    "                  , np.max(trainData.seq_item_description.max())\n",
    "                  , np.max(testData.seq_item_description.max())])+2\n",
    "MAX_CATEGORY1 = np.max([trainData.category1.max(), testData.category1.max()])+1\n",
    "MAX_CATEGORY2 = np.max([trainData.category2.max(), testData.category2.max()])+1\n",
    "MAX_CATEGORY3 = np.max([trainData.category3.max(), testData.category3.max()])+1\n",
    "\n",
    "MAX_BRAND = np.max([trainData.brand_name.max(), testData.brand_name.max()])+1\n",
    "MAX_CONDITION = np.max([trainData.item_condition_id.max(), testData.item_condition_id.max()])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "93500276-9677-46e6-9256-a0b4ee9301a9",
    "_uuid": "f87212e41f7fc03180879249eb5aa9ca5c952c41",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData[\"target\"] = np.log(trainData.price+1)\n",
    "target_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "trainData[\"target\"] = target_scaler.fit_transform(trainData.target.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6d9614a7-db37-427b-bb8e-6a1b1a81f0f9",
    "_uuid": "93f3776f02767411a9db7b6eec0b077563d72e41",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#EXTRACT DEVELOPTMENT TEST\n",
    "dtrain, dvalid = train_test_split(trainData, random_state=123, train_size=0.99)\n",
    "print(dtrain.shape)\n",
    "print(dvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cb50a6b9-cced-433f-8bea-f0fbb28ae79a",
    "_uuid": "57c42a15540d567f0feafa84b971bd0b79d23b11",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KERAS DATA DEFINITION\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def get_keras_data(dataset):\n",
    "    X = {\n",
    "        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ)\n",
    "        ,'item_desc': pad_sequences(dataset.seq_item_description, maxlen=MAX_ITEM_DESC_SEQ)\n",
    "        ,'brand_name': np.array(dataset.brand_name)\n",
    "        ,'category1': np.array(dataset.category1)\n",
    "        ,'category2': np.array(dataset.category2)\n",
    "        ,'category3': np.array(dataset.category3)\n",
    "        ,'item_condition': np.array(dataset.item_condition_id)\n",
    "        ,'num_vars': np.array(dataset[[\"shipping\"]])\n",
    "    }\n",
    "    return X\n",
    "\n",
    "X_train = get_keras_data(dtrain)\n",
    "X_valid = get_keras_data(dvalid)\n",
    "X_test = get_keras_data(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a2debc4a-bdbe-4999-b93c-da1139fadada",
    "_uuid": "5d03671b917cc1582f1922bcb55347cc81c094aa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KERAS MODEL DEFINITION\n",
    "from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras import backend as K\n",
    "def get_model():\n",
    "    #params\n",
    "    dr_r = 0.1\n",
    "    \n",
    "    #Inputs\n",
    "    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n",
    "    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n",
    "    brand_name = Input(shape=[1], name=\"brand_name\")\n",
    "    category1 = Input(shape=[1], name=\"category1\")\n",
    "    category2 = Input(shape=[1], name=\"category2\")\n",
    "    category3 = Input(shape=[1], name=\"category3\")\n",
    "    item_condition = Input(shape=[1], name=\"item_condition\")\n",
    "    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n",
    "    \n",
    "    #Embeddings layers\n",
    "    emb_name = Embedding(MAX_TEXT, 10)(name)\n",
    "    emb_item_desc = Embedding(MAX_TEXT, 50)(item_desc)\n",
    "    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n",
    "    emb_category1 = Embedding(MAX_CATEGORY1, 10)(category1)\n",
    "    emb_category2 = Embedding(MAX_CATEGORY2, 10)(category2)\n",
    "    emb_category3 = Embedding(MAX_CATEGORY3, 10)(category3)\n",
    "    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n",
    "    \n",
    "    cnn_layer1 = Conv1D(64,2,activation = 'relu', strides=1, padding='valid')(emb_item_desc)\n",
    "    pooling_layer1 = MaxPooling1D(2)(cnn_layer1)\n",
    "    cnn_layer2 = Conv1D(32,2,activation = 'relu', strides=1, padding='valid')(emb_name)\n",
    "    pooling_layer2 = MaxPooling1D(2)(cnn_layer2)\n",
    "    #main layer\n",
    "    main_l = concatenate([\n",
    "        Flatten() (emb_brand_name)\n",
    "        , Flatten() (emb_category1)\n",
    "        , Flatten() (emb_category2)\n",
    "        , Flatten() (emb_category3)\n",
    "        , Flatten() (emb_item_condition)\n",
    "        ,Flatten() (pooling_layer1)\n",
    "        ,Flatten() (pooling_layer2)\n",
    "        , num_vars\n",
    "    ])\n",
    "    main_l = Dropout(dr_r) (Dense(128) (main_l))\n",
    "    main_l = Dropout(dr_r) (Dense(64) (main_l))\n",
    "    \n",
    "    #output\n",
    "    output = Dense(1, activation=\"linear\") (main_l)\n",
    "    \n",
    "    #model\n",
    "    model = Model([name, item_desc, brand_name\n",
    "                   , category1 , category2 , category3, item_condition, num_vars], output)\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "    \n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4a8e72fa-ff1d-4126-894b-1fced84eee29",
    "_uuid": "c3955b216489ce6fbe8dc69acba83f1addeda437",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FITTING THE MODEL\n",
    "BATCH_SIZE = 20000\n",
    "epochs = 5\n",
    "\n",
    "model = get_model()\n",
    "model.fit(X_train, dtrain.target, epochs=epochs, batch_size=BATCH_SIZE\n",
    "          , validation_data=(X_valid, dvalid.target)\n",
    "          , verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bf50a444-fb0e-4927-b4c0-9154c5af18a8",
    "_uuid": "33333040d8cc721bea3469ce37127d1fd9cd0a9c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "predict = target_scaler.inverse_transform(predict)\n",
    "predict = np.exp(predict)-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1140af1a-eba2-4688-94a1-99b45be7f9f4",
    "_kg_hide-output": true,
    "_uuid": "83a9a6c455e4f59ef730c1307d7aeba48c294a98",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = testData[[\"test_id\"]]\n",
    "submission[\"price\"] = predict\n",
    "submission.to_csv(\"cnn_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
